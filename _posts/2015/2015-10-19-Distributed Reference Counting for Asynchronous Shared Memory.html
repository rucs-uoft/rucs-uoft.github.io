---
layout: post
title: "Distributed Reference Counting for Asynchronous Shared Memory"
author: "Dan Plyukhin"
supervisors: "Professor Faith Ellen"
category: "Theory of Computation"
permalink: /theory-of-computation/distributed-reference-counting-for-asynchronous-shared-memory
year: "2015"
---

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="introduction">I. Introduction</h1>
<p>The problem of memory reclamation in data structures is one of releasing ownership of nodes that will no longer be accessed. This may be automated with garbage collection, but it is not always possible or efficient to do so. Hence manual memory reclamation schemes are sometimes more desirable. A popular example is reference counting, where each node <span class="math inline">\(N\)</span> stores a count <span class="math inline">\(N.rc\)</span> of the number of incoming pointers; when <span class="math inline">\(N.rc=0\)</span>, node <span class="math inline">\(N\)</span> is unreachable and may be reclaimed.</p>
<p>Suppose that multiple processes operate on a data structure, executing at various speeds. Reference counting is nontrivial in this model: A process <span class="math inline">\(P\)</span> at node <span class="math inline">\(N\)</span> intending to increment the reference count of node <span class="math inline">\(M\)</span>, pointed to by <span class="math inline">\(N.next\)</span>, must read the pointer value before incrementing the count. During the interval between these two instructions, another process <span class="math inline">\(Q\)</span> may modify <span class="math inline">\(N.next\)</span> and decrement <span class="math inline">\(M\)</span>’s reference count. If <span class="math inline">\(Q\)</span> resets <span class="math inline">\(M.rc\)</span> to 0, then it may reclaim <span class="math inline">\(M\)</span> before <span class="math inline">\(P\)</span> can increment the count.</p>
<p>There are many memory reclamation schemes, each with advantages and disadvantages. An excellent survey can be found in <span class="citation">[1]</span>. A distributed variant of reference counting was introduced by Lee <span class="citation">[2]</span> for a particular data structure, and the goal of this research was to generalize it to arbitrary shared data structures. The result is efficient and applicable in the same contexts as ordinary reference counting.</p>

<p class="algorithm">
    <p><b>function</b><em> fetch-and-add </em><span class="math inline">\( (\mathcal{O}, i)\)</span></p>
    <blockquote>
        <p><span class="math inline">\(x \gets \mathcal{O}\)</span></p>
        <p><span class="math inline">\(\mathcal{O} \gets x + i\)</span></p>
        <p><b>return</b> <span class="math inline">\(x\)</span></p>
    </blockquote>
    <br/>
</p>

<p class="algorithm">
    <p><b>function</b><em> fetch-and-store </em><span class="math inline">\( (\mathcal{O}, y)\)</span></p>
    <blockquote>
        <p><span class="math inline">\(x \gets \mathcal{O}\)</span></p>
        <p><span class="math inline">\(\mathcal{O} \gets y\)</span></p>
        <p><b>return</b> <span class="math inline">\(x\)</span></p>
    </blockquote>
    <br/>
</p>

<p class="algorithm">
    <p><b>function</b><em> compare-and-swap </em><span class="math inline">\( (\mathcal{O}, old, new)\)</span></p>
    <blockquote>
        <p><b>if </b><span class="math inline">\( \mathcal{O} = old \)</span><b> then</b></p>
        <blockquote>
            <p><p><span class="math inline">\(\mathcal{O} \gets new\)</span></p>
            <p><b>return</b> <span class="math inline">\(T\)</span></p>
        </blockquote>
        <p><b>return</b> <span class="math inline">\(F\)</span></p>
    </blockquote>
</p>

<h1 id="the-asynchronous-shared-memory-model">II. The asynchronous shared memory model</h1>
<p>We assume a model in which <span class="math inline">\(n\)</span> processes operate on a data structure, communicating only using shared memory. Processes may be asynchronous, meaning they can be delayed arbitrarily after any instruction and are prone to crash failure, possibly leaving their operation uncompleted.</p>
<p>Data structures in this model use <em>atomic primitives</em>: instructions that consist of multiple steps, but that the hardware performs all at once. For example, <em>fetch-and-add</em> (see Figure 1) adds to the value in a word and returns its previous value. Suppose that two processes try to concurrently increment a counter <span class="math inline">\(C\)</span> by reading its value and then writing to it. It is possible that both processes read 0, then both write 1. Using <em>fetch-and-add</em>(<span class="math inline">\(C,1\)</span>), it is guaranteed that each process increments the value, one writing 1 and the other writing 2.</p>
<p>We will also consider <em>fetch-and-store</em>, which writes a value to a word and returns its old value, and <em>compare-and-swap</em> (CAS), which writes <span class="math inline">\(new\)</span> to a word if its current value is <span class="math inline">\(old\)</span> (see Figure 1); we shall always assume that <span class="math inline">\(new \neq old\)</span>.</p>
<p>We also make the distinction between pointers stored in shared memory, which we call <em>references</em>, and <em>local pointers</em>, which are stored in processes’ local memory. A node is <em>safe</em> for process <span class="math inline">\(P\)</span> if the reclamation scheme guarantees that the node will not be reclaimed before <span class="math inline">\(P\)</span> takes its next step. A local pointer is <em>safe</em> for <span class="math inline">\(P\)</span> if the node it points to is safe for <span class="math inline">\(P\)</span>.</p>

<h1 id="distributed-reference-counting">III. Distributed reference counting</h1>
<p>Detlefs et al. <span class="citation">[3]</span> gave a simple reference counting scheme for our model using the double compare-and-swap (DCAS) primitive, which atomically invokes CAS on two separate memory locations. This is used to increment the reference count of a node, provided that a particular reference continues pointing to it during the operation. However, DCAS is not commonly provided in hardware. Herlihy et al. <span class="citation">[4]</span> modified this scheme so it does not use DCAS, but their solution also requires another memory reclamation scheme.</p>
<p>Lee <span class="citation">[2]</span> provides an efficient alternative. Given a node <span class="math inline">\(N\)</span>, its <em>original reference count</em>, <span class="math inline">\(N.orc\)</span>, maintains the number of incoming references. Let <span class="math inline">\(R_N\)</span> be the set of all references pointing to <span class="math inline">\(N\)</span>. Every <span class="math inline">\(r \in R_N\)</span> is augmented with a <em>proactive reference count</em>, <span class="math inline">\(r.prc\)</span>, which counts safe local pointers. The location pointed to by the reference is stored in <span class="math inline">\(r.ptr\)</span>. To make <span class="math inline">\(N\)</span> safe, a process atomically increments <span class="math inline">\(r.prc\)</span> and reads <span class="math inline">\(r.ptr\)</span>. This is implemented by storing <span class="math inline">\(r.prc\)</span> and <span class="math inline">\(r.ptr\)</span> in the same word and invoking <em>fetch-and-add(r, 1)</em>, which returns <span class="math inline">\((r.ptr,
r.prc)\)</span>. Lastly, the node maintains a <em>distributed reference count</em>, <span class="math inline">\(N.drc\)</span>, such that <span class="math display">\[N.drc + \sum_{r \in R_N} r.prc = \text{# of safe local pointers to
  } N.\]</span> When local pointers and references to <span class="math inline">\(N\)</span> are changed, this invariant must be maintained. When a process releases a safe local pointer to <span class="math inline">\(N\)</span>, it decrements <span class="math inline">\(N.drc\)</span>. When it deletes a reference to <span class="math inline">\(r\)</span>, it atomically adds <span class="math inline">\(r.prc\)</span> to <span class="math inline">\(N.drc\)</span> and decrements <span class="math inline">\(N.orc\)</span> (to satisfy the definition of <span class="math inline">\(N.orc\)</span>). To do this atomically, <span class="math inline">\(N.drc\)</span> and <span class="math inline">\(N.orc\)</span> are stored in the same word, and modified using <em>fetch-and-add</em>. It follows that when <span class="math inline">\(N.orc = N.drc = 0\)</span>, node <span class="math inline">\(N\)</span> will never again be accessed and may be reclaimed.</p>
<p>Lee <span class="citation">[2]</span> uses this approach for an in-tree that updates references only using <em>fetch-and-store</em>. In the full version of this paper, we prove that the scheme generalizes to arbitrary data structures using <em>fetch-and-store</em> or <em>write</em> operations.</p>

<h1 id="a-further-generalization">IV. A further generalization</h1>
<p>Lee’s method does not work for data structures that apply CAS to references. Suppose a process intends to change a reference <span class="math inline">\(r\)</span> from <span class="math inline">\(oldptr\)</span> to <span class="math inline">\(newptr\)</span>. After reading <span class="math inline">\((oldptr,oldprc)\)</span> from <span class="math inline">\(r\)</span>, it would then execute CAS<span class="math inline">\((r,(oldptr,oldprc),(newptr,0))\)</span>. However, this would fail if another process incremented <span class="math inline">\(r.prc\)</span> between the read and CAS steps. This violates the specification that original CAS fails only if the reference changes value. One countermeasure is to repeatedly read <span class="math inline">\(r\)</span> and then execute CAS until either <span class="math inline">\(r.prc\)</span> remains fixed or <span class="math inline">\(r.ptr\)</span> is changed. However, it is not guaranteed that the operation will ever complete.</p>
<p>To ensure that the operation always completes, we equip each reference with an additional <span class="math inline">\(pid\)</span> variable stored in a separate word. Let processes <span class="math inline">\(P_i,P_j\)</span> have identifiers <span class="math inline">\(i,j\)</span> respectively. Suppose <span class="math inline">\(P_i\)</span> intends to CAS <span class="math inline">\(r.ptr\)</span> from <span class="math inline">\(old\)</span> to <span class="math inline">\(new\)</span>. If it reads that <span class="math inline">\(r.ptr \neq old\)</span>, it may return F immediately. Otherwise it invokes CAS<span class="math inline">\((r.pid,NULL,i)\)</span>. If this succeeds, then every other process that wants to access <span class="math inline">\(r\)</span> must help <span class="math inline">\(P_i\)</span> complete its CAS before continuing with their own operations.</p>
<p>Suppose instead that CAS<span class="math inline">\((r.pid,NULL,i)\)</span> fails with <span class="math inline">\(r.pid=j\)</span>. Then after helping <span class="math inline">\(P_j\)</span>, process <span class="math inline">\(P_i\)</span> may conclude that <span class="math inline">\(r.ptr \neq old\)</span>, and therefore that its operation will inevitably fail. Hence it may return F without trying to set <span class="math inline">\(r.pid\)</span> again. Consequently, <span class="math inline">\(P_i\)</span>’s operation always terminates.</p>

<h4 id="acknowledgements">Acknowledgements</h4>
<p class="acknowledgements">I owe great thanks to my supervisor Professor Faith Ellen, and to Kenneth D. Hoover, Yuanhao Wei, and Alexey Shablygin for fruitful discussions. This work was supported by NSERC.</p>

<h4>References</h4>
<div class="references">
    <div id="ref-brown2015reclaiming">
    <p>[1] T. Brown, "Reclaiming memory for lock-free data structures: There has to be a better way," <em>Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing (PODC)</em>, pp. 261–270, 2015.</p>
    </div>
    <div id="ref-lee2010fast">
    <p>[2] H. Lee, "Fast local-spin abortable mutual exclusion with bounded space," <em>Proceedings of the 14th International Conference on Principles of Distributed Systems (OPODIS)</em>, pp. 364–379, 2010.</p>
    </div>
    <div id="ref-detlefs2002lock">
    <p>[3] D. L. Detlefs, P. A. Martin, M. Moir, and G. L. Steele Jr, "Lock-free reference counting," <em>Distributed Computing</em>, vol. 15, no. 4, pp. 255–271, 2002.</p>
    </div>
    <div id="ref-herlihy2005nonblocking">
    <p>[4] M. Herlihy, V. Luchangco, P. Martin, and M. Moir, "Nonblocking memory management support for dynamic-sized data structures," <em>ACM Transactions on Computer Systems (TOCS)</em>, vol. 23, no. 2, pp. 146–196, 2005.</p>
</div>
